{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "07_nlp_lstm_gru.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukinaga/ai_programming/blob/main/lecture_09/07_nlp_lstm_gru.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfL5Gt8uEyHl"
      },
      "source": [
        "#  LSTM、GRUによる自然言語処理\n",
        "LSTM、GRUを使って、文書の自動作成を行います。  \n",
        "今回も、江戸川乱歩の「怪人二十面相」を学習データに使い、乱歩風の文章を自動生成します。  \n",
        "以前にSimpleRNNを扱った時と同じように、文章における文字の並びを時系列データと捉えて、次の文字を予測するように訓練します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYr124ufEyHp"
      },
      "source": [
        "## テキストデータの前処理\n",
        "今回は、LSTM、GRU共に学習データとして青空文庫の「怪人二十面相」を使います。  \n",
        "最初に、テキストデータに前処理を行います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c4Mz62FEyHq"
      },
      "source": [
        "import re\n",
        "\n",
        "with open(\"kaijin_nijumenso.txt\", mode=\"r\", encoding=\"utf-8\") as f:  # ファイルの読み込み\n",
        "    text_original = f.read()\n",
        "\n",
        "text = re.sub(\"《[^》]+》\", \"\", text_original) # ルビの削除\n",
        "text = re.sub(\"［[^］]+］\", \"\", text) # 読みの注意の削除\n",
        "text = re.sub(\"[｜ 　]\", \"\", text) # | と全角半角スペースの削除\n",
        "print(\"文字数\", len(text))  # len() で文字列の文字数も取得可能"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEKlSq7bEyHs"
      },
      "source": [
        "## 各設定\n",
        "LSTM、GRU共通の各設定です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNamQpH_EyHs"
      },
      "source": [
        "n_rnn = 10  # 時系列の数\n",
        "batch_size = 128\n",
        "epochs = 60\n",
        "n_mid = 256  # 中間層のニューロン数"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvEzD6UuEyHs"
      },
      "source": [
        "## 文字のベクトル化\n",
        "各文字をone-hot表現で表し、時系列の入力データおよび正解データを作成します。  \n",
        "今回もRNNの最後の時刻の出力のみ利用するので、最後の出力に対応する正解のみ必要になります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPcAnK5JEyHt"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# インデックスと文字で辞書を作成\n",
        "chars = sorted(list(set(text)))  # setで文字の重複をなくし、各文字をリストに格納する\n",
        "print(\"文字数（重複無し）\", len(chars))\n",
        "char_indices = {}  # 文字がキーでインデックスが値\n",
        "for i, char in enumerate(chars):\n",
        "    char_indices[char] = i\n",
        "indices_char = {}  # インデックスがキーで文字が値\n",
        "for i, char in enumerate(chars):\n",
        "    indices_char[i] = char\n",
        " \n",
        "# 時系列データと、それから予測すべき文字を取り出します\n",
        "time_chars = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - n_rnn):\n",
        "    time_chars.append(text[i: i + n_rnn])\n",
        "    next_chars.append(text[i + n_rnn])\n",
        " \n",
        "# 入力と正解をone-hot表現で表します\n",
        "x = np.zeros((len(time_chars), n_rnn, len(chars)), dtype=np.bool)\n",
        "t = np.zeros((len(time_chars), len(chars)), dtype=np.bool)\n",
        "for i, t_cs in enumerate(time_chars):\n",
        "    t[i, char_indices[next_chars[i]]] = 1  # 正解をone-hot表現で表す\n",
        "    for j, char in enumerate(t_cs):\n",
        "        x[i, j, char_indices[char]] = 1  # 入力をone-hot表現で表す\n",
        "        \n",
        "print(\"xの形状\", x.shape)\n",
        "print(\"tの形状\", t.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib1m8lahEyHu"
      },
      "source": [
        "## LSTMモデルの構築\n",
        "Kerasを使ってLSTMを構築します。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9afcF_cEyHu"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(LSTM(n_mid, input_shape=(n_rnn, len(chars))))\n",
        "model_lstm.add(Dense(len(chars), activation=\"softmax\"))\n",
        "model_lstm.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n",
        "print(model_lstm.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GSTELfZEyHv"
      },
      "source": [
        "## 文書生成用の関数\n",
        "各エポックの終了後、文章を生成するための関数を記述します。  \n",
        "LambdaCallbackを使って、エポック終了時に実行される関数を設定します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k0MjxniEyHv"
      },
      "source": [
        "from keras.callbacks import LambdaCallback\n",
        " \n",
        "def on_epoch_end(epoch, logs):\n",
        "    print(\"エポック: \", epoch)\n",
        "\n",
        "    beta = 5  # 確率分布を調整する定数\n",
        "    prev_text = text[0:n_rnn]  # 入力に使う文字\n",
        "    created_text = prev_text  # 生成されるテキスト\n",
        "    \n",
        "    print(\"シード: \", created_text)\n",
        "\n",
        "    for i in range(400):\n",
        "        # 入力をone-hot表現に\n",
        "        x_pred = np.zeros((1, n_rnn, len(chars)))\n",
        "        for j, char in enumerate(prev_text):\n",
        "            x_pred[0, j, char_indices[char]] = 1\n",
        "        \n",
        "        # 予測を行い、次の文字を得る\n",
        "        y = model.predict(x_pred)\n",
        "        p_power = y[0] ** beta  # 確率分布の調整\n",
        "        next_index = np.random.choice(len(p_power), p=p_power/np.sum(p_power))        \n",
        "        next_char = indices_char[next_index]\n",
        "\n",
        "        created_text += next_char\n",
        "        prev_text = prev_text[1:] + next_char\n",
        "\n",
        "    print(created_text)\n",
        "    print()\n",
        "\n",
        "# エポック終了後に実行される関数を設定\n",
        "epock_end_callback= LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-1dhfBEEyHw"
      },
      "source": [
        "## 学習\n",
        "構築したLSTMを使って、学習を行います。  \n",
        "fit( )メソッドでコールバックの設定をし、エポック終了後に関数が呼ばれるようにします。  \n",
        "学習には数時間かかるので、時間のない方はエポック数を少なくして実行しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYg0_RsREyHw"
      },
      "source": [
        "model = model_lstm\n",
        "history_lstm = model_lstm.fit(x, t,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    callbacks=[epock_end_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbLMFIGPEyHx"
      },
      "source": [
        "次第に、乱歩のような文体の文章が生成されていくことが確認できますね。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng9i978QEyHx"
      },
      "source": [
        "## GRUモデルの構築\n",
        "Kerasを使ってGRUを構築します。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dqc7x1HNEyHx"
      },
      "source": [
        "from keras.layers import GRU\n",
        "\n",
        "model_gru = Sequential()\n",
        "model_gru.add(GRU(n_mid, input_shape=(n_rnn, len(chars))))\n",
        "model_gru.add(Dense(len(chars), activation=\"softmax\"))\n",
        "model_gru.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n",
        "print(model_gru.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baqxCuJqEyHy"
      },
      "source": [
        "## 学習\n",
        "構築したGRUを使って、学習を行います。  \n",
        "fit( )メソッドでコールバックの設定をし、エポック終了後に関数が呼ばれるようにします。  \n",
        "LSTMと同じく学習には数時間程度かかるので、時間のない方はエポック数を少なくして実行しましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGwVzfj6EyHy"
      },
      "source": [
        "model = model_gru\n",
        "history_gru = model_gru.fit(x, t,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    callbacks=[epock_end_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9qTorOpEyHy"
      },
      "source": [
        "GRUでも、乱歩のような文体の文章が生成されていくことが確認できますね。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch_r00gjEyHy"
      },
      "source": [
        "## 学習の推移\n",
        "誤差の推移を確認します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Snmy1s7EyHy"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_lstm = history_lstm.history['loss']\n",
        "loss_gru = history_gru.history['loss']\n",
        "\n",
        "plt.plot(np.arange(len(loss_lstm)), loss_lstm, label=\"LSTM\")\n",
        "plt.plot(np.arange(len(loss_gru)), loss_gru, label=\"GRU\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMQGyjx7EyHz"
      },
      "source": [
        "両者ともに、誤差が収束していますね。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6DB_dujEyHz"
      },
      "source": [
        "## 生成された文章を比較\n",
        "LSTM、GRU、それぞれで生成された文章を比較してみましょう。\n",
        "\n",
        "#### LSTM\n",
        "Epoch 60/60  \n",
        "110313/110313 [==============================] - 126s 1ms/step - loss: 0.1493  \n",
        "エポック:  59  \n",
        "シード:  そのころ、東京中の町  \n",
        "そのころ、東京中の町という町、家という家では、ふたり以上の人が顔をあわせさえすれば、まるでお天気のあいさつでもするように、怪人「二十面相」のうわさばかりしているというのも、じつは、こわばかくで、ぼくをひっててやけるかもですね。その部屋にはあきません。おしてお目をとりだっしましたけれど、賊は、あいつは邸内が三人の前まになにのをつかれたというのです。  \n",
        "それでも、外見らめてあるというからだ。手がありません。  \n",
        "「二十面相のやつ、今夜ですよ。手紙のんのは、小林少年の苦心に、子どおりたんださい。このあばかな声ですから、そのじゃにともの見れています。もしかむこんわけは、何かしこそこしだ。こぎだから、ばかりタとしておね。ぼくはそうですね。あれは何かわらのことを、かけつけているのでしょうか。またかと大き賊の部下があのがわらをじて、じて、ゆっくりむずねばながってもたじつめて、庭園のことへはそうかさえしまいました。  \n",
        "「ああ、よかっ  \n",
        "\n",
        "#### GRU\n",
        "Epoch 60/60  \n",
        "110313/110313 [==============================] - 104s 942us/step - loss: 0.2000  \n",
        "エポック:  59  \n",
        "シード:  そのころ、東京中の町  \n",
        "そのころ、東京中の町という町、家という家では、ふたり以上の人が顔をあわせさ。すると、その下から黒々とした頭があらわれました。つぎには、おとうさんにちょっと会われてください。ぼくは少しやらくるようにしますと、電燈がよいつけて、主人の壮二君と、赤井寅三に、「二十面相」たいには、十分かしたというのかね。」  \n",
        "「ええ、おくびょうのようですけれど、なんだかそんな気がするのです。」  \n",
        "「だが、そんなことはないかなければよりませんでした。  \n",
        "「ほかのものならばかまわない。ダイヤなぞお金さえ出せば手にこぎってはたかぬかまりませんだ。それをしゃべりっていって、目的をはたしてしまったのですから、むりもないことです。  \n",
        "「いや、なんとも申しあげようもありません。二十面相がこれほどの腕まえとは知りませんでした。相手がですよ。それが二十面相の部下に会ったのか。いったい、どこで？どうして？」  \n",
        "さすがの警官はといって、やぶやから手もにもおわしてや  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e67VFX2EyH0"
      },
      "source": [
        "今回のケースでは、GRUで生成された文章の方が自然に見えますね。  \n",
        "興味のある方は、様々な条件をトライし、より自然な文章の生成にトライしてみましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-04zYY5TEyH0"
      },
      "source": [
        "## 課題\n",
        "青空文庫の「銀河鉄道の夜」を読み込み、宮沢賢治風の文章をLSTM、もしくはGRUで自動生成してみましょう。  \n",
        "このノートブックと同じフォルダに、以下のファイルがあります。  \n",
        "gingatetsudono_yoru.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOl7fP0LEyH0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}