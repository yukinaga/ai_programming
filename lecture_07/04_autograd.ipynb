{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_autograd.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukinaga/ai_programming/blob/main/lecture_07/04_autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FgvuyIa6oBQ"
      },
      "source": [
        "# 自動微分\n",
        "自動微分により、ある値の微小変化が結果に与える影響を自動で計算することができます。   \n",
        "参考: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpu9CjtSPaP8"
      },
      "source": [
        "## 自動微分の開始\n",
        "Tensorは、requires_grad属性をTrueに設定することで計算過程が記録されるようになります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3loxKiFXYylA"
      },
      "source": [
        "import torch\n",
        "\n",
        "x = torch.ones(2, 3, requires_grad=True)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0aISTAIsbKK"
      },
      "source": [
        "## Tensorの演算と自動微分\n",
        "requires_grad属性がTrueであれば、演算によりgrad_fnが記録されます。  \n",
        "grad_fnは、このTensorを作った演算です。  \n",
        "以下では、`x`に足し算を行って得られた`y`のgrad_fnを表示します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQpV46k3tx3R"
      },
      "source": [
        "y = x + 2\n",
        "print(y)\n",
        "print(y.grad_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9amPZxOid4w5"
      },
      "source": [
        "掛け算、mean関数などの演算も、grad_fnに記録されます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKs5LFQMt4zd"
      },
      "source": [
        "z = y * 3\n",
        "print(z)\n",
        "\n",
        "out = z.mean()\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83QbuM5xf2Of"
      },
      "source": [
        "## 勾配の計算\n",
        "backwardメソッドは、逆伝播により勾配を計算します。  \n",
        "その際に、 記録されている演算と経路が使用されます。  \n",
        "以下の例では、aに2をかけてbとしていますが、backwardによりaの変化に対するbの変化の割合、すなわち勾配が計算されます。  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_HJnklKt7nt"
      },
      "source": [
        "a = torch.tensor([1.0], requires_grad=True)\n",
        "b = a * 2  # bの変化量はaの2倍\n",
        "b.backward()  # 逆伝播\n",
        "print(a.grad)  # aの勾配（aの変化に対するbの変化の割合）　　"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBcAhTCnqA1Z"
      },
      "source": [
        "より複雑な経路を持つ演算でも、backwardにより勾配を計算することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57kKqrGQt-SP"
      },
      "source": [
        "def calc(a):\n",
        "    b = a*2 + 1\n",
        "    c = b*b \n",
        "    d = c/(c + 2)\n",
        "    e = d.mean()\n",
        "    return e\n",
        "\n",
        "x = [1.0, 2.0, 3.0]\n",
        "x = torch.tensor(x, requires_grad=True)\n",
        "y = calc(x)\n",
        "y.backward()\n",
        "print(x.grad.tolist())  # xの勾配（xの各値の変化に対するyの変化の割合）　　"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRSyV5hSLZbO"
      },
      "source": [
        "xの各値付近における勾配が計算できました。  \n",
        "勾配が正しく計算できていることを確認しましょう。  \n",
        "`x`を微小変化させて、`x`の微小変化に対する`y`の微小変化の割合を求めます。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtWE7zoduAnJ"
      },
      "source": [
        "delta = 0.001  #xの微小変化\n",
        "\n",
        "x = [1.0, 2.0, 3.0]\n",
        "x = torch.tensor(x, requires_grad=True)\n",
        "y = calc(x).item()\n",
        "\n",
        "x_1 = [1.0+delta, 2.0, 3.0]\n",
        "x_1 = torch.tensor(x_1, requires_grad=True)\n",
        "y_1 = calc(x_1).item()\n",
        "\n",
        "x_2 = [1.0, 2.0+delta, 3.0]\n",
        "x_2 = torch.tensor(x_2, requires_grad=True)\n",
        "y_2 = calc(x_2).item()\n",
        "\n",
        "x_3 = [1.0, 2.0, 3.0+delta]\n",
        "x_3 = torch.tensor(x_3, requires_grad=True)\n",
        "y_3 = calc(x_3).item()\n",
        "\n",
        "# 勾配の計算\n",
        "grad_1 = (y_1 - y) / delta\n",
        "grad_2 = (y_2 - y) / delta\n",
        "grad_3 = (y_3 - y) / delta\n",
        "\n",
        "print(grad_1, grad_2, grad_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwdACN7iWQiy"
      },
      "source": [
        "`x`の微小変化を0.001という小さい値にしましたが、`y`の微小変化との割合は　backwardによる計算結果とほぼ同じになりました。  \n",
        "backwardにより正しく勾配を計算できていることが確認できました。"
      ]
    }
  ]
}